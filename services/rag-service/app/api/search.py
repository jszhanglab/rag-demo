from fastapi import APIRouter, HTTPException
from app.utils.config import API_ROUTES
from app.utils.config import settings
from app.services.search_service import SearchService
from app.services.embedding_service import EmbeddingService
from app.services.llm_service import LLMService
from app.vectorstore.chroma_repo import ChromaVectorRepository
from pydantic import BaseModel, Field
from typing import List, Optional

# Router configuration with English tags
router = APIRouter(prefix=API_ROUTES['SEARCH_DOCUMENT'], tags=["search"])

# --- Request & Response Models ---

class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1, description="The search query text entered by the user")
    # You can change the default top_k here if you want to increase context by default
    top_k: int = Field(8, ge=1, le=50, description="Number of relevant chunks to retrieve for context")
    document_id: Optional[str] = Field(None, description="Optional filter to search within a specific document")

class SearchHitResponse(BaseModel):
    chunk_id: str
    distance: float
    text: str
    document_id: str
    chunk_index: int
    start_offset: Optional[int] = None
    end_offset: Optional[int] = None
    metadata: dict

class SearchResponse(BaseModel):
    answer: str = Field(..., description="The comprehensive answer generated by the LLM based on retrieved context")
    hits: List[SearchHitResponse] = Field(..., description="The list of raw document chunks used to generate the answer")

# --- Service Initialization ---

# Initialize embedding service for vectorizing queries
embedding_service = EmbeddingService(
    model_name=settings.LLM_MODEL_DEV,
    normalize=True,
    batch_size=32,
    device=None,
)

# Initialize vector repository (ChromaDB)
vector_repo = ChromaVectorRepository(
    persist_dir=settings.VECTOR_DB_URL,
    collection_name="rag_chunks",
)

# Core search logic service
search_service = SearchService(
    embedding_service=embedding_service,
    vector_repo=vector_repo,
)

# LLM service for answer generation (Gemini)
llm_service = LLMService()

# --- API Endpoints ---

@router.post("", response_model=SearchResponse)
async def search(request: SearchRequest):
    """
    Perform semantic search and generate a natural language answer:
    1. Retrieve the most relevant text chunks from the vector database.
    2. Feed the retrieved chunks as context into the Gemini LLM.
    3. Return both the generated answer and the source chunks for transparency.
    """
    try:
        # Step 1: Vector-based semantic retrieval
        # Note: 'top_k' is passed from the request, but defaults to 8 now for better context
        hits = search_service.search(
            query=request.query, 
            top_k=request.top_k, 
            document_id=request.document_id
        )

        # Handle cases where no relevant documents are found
        if not hits:
            return SearchResponse(
                answer="I'm sorry, but no relevant information was found in the document to answer your question.",
                hits=[]
            )

        # Step 2: Extract text content from hits for LLM context
        context_texts = [hit.text for hit in hits]

        # Step 3: Generate the final answer using the LLM service
        answer = await llm_service.get_answer(query=request.query, chunks=context_texts)

        # Step 4: Assemble and return the final response
        return SearchResponse(
            answer=answer,
            hits=[
                SearchHitResponse(
                    chunk_id=hit.chunk_id,
                    distance=hit.distance,
                    text=hit.text,
                    document_id=hit.document_id,
                    chunk_index=hit.chunk_index,
                    start_offset=hit.start_offset,
                    end_offset=hit.end_offset,
                    metadata=hit.metadata
                )
                for hit in hits
            ]
        )

    except Exception as e:
        # Log the error and return a 500 status code
        print(f"Search API Error: {str(e)}")
        raise HTTPException(status_code=500, detail="An internal error occurred during the search process.")